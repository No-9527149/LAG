{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from .runner.jsbsim_runner import _t2n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect(self, step):\n",
    "    self.policy.prep_rollout()\n",
    "    values, actions, action_log_probs, rnn_states_actor, rnn_states_critic = (\n",
    "        self.policy.get_actions(\n",
    "            np.concatenate(self.buffer.obs[step]),\n",
    "            np.concatenate(self.buffer.rnn_states_actor[step]),\n",
    "            np.concatenate(self.buffer.rnn_states_critic[step]),\n",
    "            np.concatenate(self.buffer.masks[step]),\n",
    "        )\n",
    "    )\n",
    "    # split parallel data [N*M, shape] => [N, M, shape]\n",
    "    values = np.array(np.split(_t2n(values), self.n_rollout_threads))\n",
    "    actions = np.array(np.split(_t2n(actions), self.n_rollout_threads))\n",
    "    action_log_probs = np.array(\n",
    "        np.split(_t2n(action_log_probs), self.n_rollout_threads)\n",
    "    )\n",
    "    rnn_states_actor = np.array(\n",
    "        np.split(_t2n(rnn_states_actor), self.n_rollout_threads)\n",
    "    )\n",
    "    rnn_states_critic = np.array(\n",
    "        np.split(_t2n(rnn_states_critic), self.n_rollout_threads)\n",
    "    )\n",
    "    return values, actions, action_log_probs, rnn_states_actor, rnn_states_critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    " * @Author       : zzp@buaa.edu.cn\n",
    " * @Date         : 2024-11-14 14:56:49\n",
    " * @LastEditTime : 2024-11-14 15:45:26\n",
    " * @FilePath     : /LAG/notes.ipynb\n",
    " * @Description  : \n",
    "-->\n",
    "`JSBSIMRunner(): def collect(self, step)`: 这段代码定义了一个方法 collect，用于在强化学习的采样过程中收集数据。以下是对代码的逐步解析\n",
    "\n",
    "1. `@torch.no_grad()` 装饰器：\n",
    "\n",
    "   * 标记 `collect` 方法在执行时不计算梯度，这样可以节省内存和计算资源，因为在采样时不需要反向传播\n",
    "\n",
    "2. `self.policy.prep_rollout()`：\n",
    "\n",
    "   * 准备策略模型进入 `rollout`（采样）模式。通常涉及将模型设置为推理模式，比如 PyTorch 中的 `model.eval()`\n",
    "\n",
    "3. 调用 `self.policy.get_actions` 获取行动数据:\n",
    "\n",
    "   * `np.concatenate(...)`：通过拼接操作，将多个并行线程的数据在第0维（batch维度）合并，输入给策略网络\n",
    "\n",
    "   * `rnn_states_actor` 和 `rnn_states_critic`：分别是 `actor` 和 `critic` 网络的 RNN 隐状态，用于在下一个时间步保持信息的连续性\n",
    "\n",
    "4. 并行数据的分割与重组:\n",
    "\n",
    "   * `np.array(np.split(..., self.n_rollout_threads))`：这一步通过 `np.split` 将每个变量从 [N*M, shape] 形状（N 表示采样步数，M 表示并行 rollout 线程数）拆分为 [N, M, shape] 形状。这样做的目的是方便并行处理不同的 rollout 线程\n",
    "\n",
    "   * 由于之前数据是拼接的，将其分割成 [N, M, shape] 格式，其中 N 表示 rollout 线程数，M 表示批次大小（每个线程的数据量）。通过 _t2n 方法将 Tensor 转换为 numpy 数组，再用 np.split 按线程分割\n",
    "\n",
    "   * `_t2n`：将 PyTorch tensor 转换为 NumPy 数组，以便与 NumPy 操作兼容\n",
    "\n",
    "5. 返回：\n",
    "\n",
    "   * 最终返回 values, actions, action_log_probs, rnn_states_actor, rnn_states_critic，这些数据会被存储到经验回放缓冲区，用于后续的策略优化和训练\n",
    "\n",
    "总体而言，这段代码用于在并行环境中执行 rollout，采样多个线程的数据并进行分割和重组，为强化学习的下一步训练准备所需的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from .algorithms.utils.mlp import MLPBase\n",
    "from .algorithms.utils.gru import GRULayer\n",
    "from .algorithms.utils.act import ACTLayer\n",
    "from .algorithms.utils.utils import check\n",
    "\n",
    "\n",
    "class PPOActor(nn.Module):\n",
    "    def __init__(self, args, obs_space, act_space, device=torch.device(\"cpu\")):\n",
    "        super(PPOActor, self).__init__()\n",
    "        # network config\n",
    "        self.gain = args.gain\n",
    "        self.hidden_size = args.hidden_size\n",
    "        self.act_hidden_size = args.act_hidden_size\n",
    "        self.activation_id = args.activation_id\n",
    "        self.use_feature_normalization = args.use_feature_normalization\n",
    "        self.use_recurrent_policy = args.use_recurrent_policy\n",
    "        self.recurrent_hidden_size = args.recurrent_hidden_size\n",
    "        self.recurrent_hidden_layers = args.recurrent_hidden_layers\n",
    "        self.tpdv = dict(dtype=torch.float32, device=device)\n",
    "        self.use_prior = args.use_prior\n",
    "        # (1) feature extraction module\n",
    "        self.base = MLPBase(\n",
    "            obs_space,\n",
    "            self.hidden_size,\n",
    "            self.activation_id,\n",
    "            self.use_feature_normalization,\n",
    "        )\n",
    "        # (2) rnn module\n",
    "        input_size = self.base.output_size\n",
    "        if self.use_recurrent_policy:\n",
    "            self.rnn = GRULayer(\n",
    "                input_size, self.recurrent_hidden_size, self.recurrent_hidden_layers\n",
    "            )\n",
    "            input_size = self.rnn.output_size\n",
    "        # (3) act module\n",
    "        self.act = ACTLayer(\n",
    "            act_space, input_size, self.act_hidden_size, self.activation_id, self.gain\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, obs, rnn_states, masks, deterministic=False):\n",
    "        obs = check(obs).to(**self.tpdv)\n",
    "        rnn_states = check(rnn_states).to(**self.tpdv)\n",
    "        masks = check(masks).to(**self.tpdv)\n",
    "        if self.use_prior:\n",
    "            # prior knowledge for controlling shoot missile\n",
    "            attack_angle = torch.rad2deg(obs[:, 11])  # unit degree\n",
    "            distance = obs[:, 13] * 10000  # unit m\n",
    "            alpha0 = torch.full(size=(obs.shape[0], 1), fill_value=3).to(**self.tpdv)\n",
    "            beta0 = torch.full(size=(obs.shape[0], 1), fill_value=10).to(**self.tpdv)\n",
    "            alpha0[distance <= 12000] = 6\n",
    "            alpha0[distance <= 8000] = 10\n",
    "            beta0[attack_angle <= 45] = 6\n",
    "            beta0[attack_angle <= 22.5] = 3\n",
    "\n",
    "        actor_features = self.base(obs)\n",
    "\n",
    "        if self.use_recurrent_policy:\n",
    "            actor_features, rnn_states = self.rnn(actor_features, rnn_states, masks)\n",
    "\n",
    "        if self.use_prior:\n",
    "            actions, action_log_probs = self.act(\n",
    "                actor_features, deterministic, alpha0=alpha0, beta0=beta0\n",
    "            )\n",
    "        else:\n",
    "            actions, action_log_probs = self.act(actor_features, deterministic)\n",
    "\n",
    "        return actions, action_log_probs, rnn_states\n",
    "\n",
    "    def evaluate_actions(self, obs, rnn_states, action, masks, active_masks=None):\n",
    "        obs = check(obs).to(**self.tpdv)\n",
    "        rnn_states = check(rnn_states).to(**self.tpdv)\n",
    "        action = check(action).to(**self.tpdv)\n",
    "        masks = check(masks).to(**self.tpdv)\n",
    "        if self.use_prior:\n",
    "            # prior knowledge for controlling shoot missile\n",
    "            attack_angle = torch.rad2deg(obs[:, 11])  # unit degree\n",
    "            distance = obs[:, 13] * 10000  # unit m\n",
    "            alpha0 = torch.full(size=(obs.shape[0], 1), fill_value=3).to(**self.tpdv)\n",
    "            beta0 = torch.full(size=(obs.shape[0], 1), fill_value=10).to(**self.tpdv)\n",
    "            alpha0[distance <= 12000] = 6\n",
    "            alpha0[distance <= 8000] = 10\n",
    "            beta0[attack_angle <= 45] = 6\n",
    "            beta0[attack_angle <= 22.5] = 3\n",
    "\n",
    "        if active_masks is not None:\n",
    "            active_masks = check(active_masks).to(**self.tpdv)\n",
    "\n",
    "        actor_features = self.base(obs)\n",
    "\n",
    "        if self.use_recurrent_policy:\n",
    "            actor_features, rnn_states = self.rnn(actor_features, rnn_states, masks)\n",
    "\n",
    "        if self.use_prior:\n",
    "            action_log_probs, dist_entropy = self.act.evaluate_actions(\n",
    "                actor_features, action, active_masks, alpha0=alpha0, beta0=beta0\n",
    "            )\n",
    "        else:\n",
    "            action_log_probs, dist_entropy = self.act.evaluate_actions(\n",
    "                actor_features, action, active_masks\n",
    "            )\n",
    "\n",
    "        return action_log_probs, dist_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了一个 PPOActor 类，它是一个强化学习模型的 actor 模块，基于 Proximal Policy Optimization (PPO) 算法。该模块主要实现策略网络的结构和前向传播过程，并支持特征提取、循环神经网络（RNN）、动作输出和先验知识。下面是对各部分的详细解析：\n",
    "\n",
    "# 初始化方法 __init__\n",
    "\n",
    "* 初始化参数：该方法接受模型的参数 args，观察空间 obs_space，动作空间 act_space，以及计算设备 device（默认为 CPU）。\n",
    "\n",
    "* 网络配置：从 args 中获取模型的超参数和配置，包括增益 gain、隐藏层大小 hidden_size、激活函数选择 activation_id、是否使用特征标准化 use_feature_normalization、是否使用循环策略 use_recurrent_policy、以及先验知识 use_prior\n",
    "\n",
    "* 特征提取模块 (self.base)：调用 MLPBase 初始化特征提取网络，处理观察输入 obs_space 后输出隐藏层特征\n",
    "\n",
    "* RNN 模块 (self.rnn)：如果启用了循环策略 use_recurrent_policy，则创建 GRU 层（即 GRULayer），用于处理时间序列的特征提取\n",
    "\n",
    "* 动作输出模块 (self.act)：ACTLayer 生成模型的动作输出，输入为特征提取后的数据，输出相应的动作空间 act_space 中的动作。\n",
    "\n",
    "# 前向传播方法 forward\n",
    "\n",
    "* 输入处理：检查并将 obs、rnn_states、masks 转换为指定数据类型和设备，以保证输入数据的一致性\n",
    "\n",
    "* 先验知识应用：\n",
    "\n",
    "  * 如果 use_prior 为真，则根据先验知识调整策略网络的输出。这里的先验知识基于攻击角度 attack_angle 和距离 distance，通过调整参数 alpha0 和 beta0 控制模型对发射导弹的策略倾向：\n",
    "    * 当距离较近时增大 alpha0，使得动作更趋向发射导弹；\n",
    "    * 当攻击角度较小（即更接近正前方）时，减小 beta0，增加导弹发射的可能性\n",
    "\n",
    "* 特征提取：将观察输入 obs 通过 MLPBase 提取特征 actor_features\n",
    "\n",
    "* RNN 特征提取：\n",
    "  * 如果使用了循环策略，将特征和 RNN 状态 rnn_states 输入到 GRU 层进行时间序列处理，并更新 RNN 隐状态\n",
    "\n",
    "* 动作输出：\n",
    "  * 根据 use_prior 的设定，调用 ACTLayer 获取最终的动作 actions 和动作的对数概率 action_log_probs。如果启用了先验知识，则会将 alpha0 和 beta0 作为参数传递给 ACTLayer，否则直接使用提取的特征进行输出\n",
    "\n",
    "* 输出： 返回动作 actions，动作的对数概率 action_log_probs，以及更新后的 RNN 隐状态 rnn_states\n",
    "\n",
    "# 总结\n",
    "\n",
    "PPOActor 是一个基于多层感知器 (MLP) 和 GRU 的 PPO 策略网络。它的结构主要由特征提取模块、RNN 模块和动作输出模块组成。PPOActor 在 forward 中整合了先验知识，可以根据特定条件（如攻击角度和距离）动态调整输出策略。这种设计适用于需要长序列特征和先验知识辅助的复杂任务，例如导弹发射决策的强化学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def collect(self, step):\n",
    "    self.policy.prep_rollout()\n",
    "    values, actions, action_log_probs, rnn_states_actor, rnn_states_critic = (\n",
    "        self.policy.get_actions(\n",
    "            np.concatenate(self.buffer.obs[step]),\n",
    "            np.concatenate(self.buffer.rnn_states_actor[step]),\n",
    "            np.concatenate(self.buffer.rnn_states_critic[step]),\n",
    "            np.concatenate(self.buffer.masks[step]),\n",
    "        )\n",
    "    )\n",
    "    # split parallel data [N*M, shape] => [N, M, shape]\n",
    "    values = np.array(np.split(_t2n(values), self.n_rollout_threads))\n",
    "    actions = np.array(np.split(_t2n(actions), self.n_rollout_threads))\n",
    "    action_log_probs = np.array(\n",
    "        np.split(_t2n(action_log_probs), self.n_rollout_threads)\n",
    "    )\n",
    "    rnn_states_actor = np.array(\n",
    "        np.split(_t2n(rnn_states_actor), self.n_rollout_threads)\n",
    "    )\n",
    "    rnn_states_critic = np.array(\n",
    "        np.split(_t2n(rnn_states_critic), self.n_rollout_threads)\n",
    "    )\n",
    "\n",
    "    # [Selfplay] get actions of opponent policy\n",
    "    opponent_actions = np.zeros_like(actions)\n",
    "    for policy_idx, policy in enumerate(self.opponent_policy):\n",
    "        env_idx = self.opponent_env_split[policy_idx]\n",
    "        opponent_action, opponent_rnn_states = policy.act(\n",
    "            np.concatenate(self.opponent_obs[env_idx]),\n",
    "            np.concatenate(self.opponent_rnn_states[env_idx]),\n",
    "            np.concatenate(self.opponent_masks[env_idx]),\n",
    "        )\n",
    "        opponent_actions[env_idx] = np.array(\n",
    "            np.split(_t2n(opponent_action), len(env_idx))\n",
    "        )\n",
    "        self.opponent_rnn_states[env_idx] = np.array(\n",
    "            np.split(_t2n(opponent_rnn_states), len(env_idx))\n",
    "        )\n",
    "    actions = np.concatenate((actions, opponent_actions), axis=1)\n",
    "\n",
    "    return values, actions, action_log_probs, rnn_states_actor, rnn_states_critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SelfplayJSBSimRunner`\n",
    "\n",
    "# [自对弈] 获取对手策略的动作数据\n",
    "\n",
    "* 如果使用自对弈，该部分会依次遍历每个对手策略 self.opponent_policy，并获取对手的动作 opponent_action 和 RNN 状态 opponent_rnn_states\n",
    "  * 单次选取的对手只有一个，即 num_opponents == 1\n",
    "  * opponent_policy (__type__: list) 实际上列表中只有一个，所有的 opponent 使用的都是同一种 policy，即 PPOPolicy\n",
    "  * opponent_env_split (__type__: list): 将 n_rollout_threads 分成若干份，每一份包含 len(opponent_policy) 个thread，实际上 32 个 threads 也就分成了 32 份\n",
    "  * 这样就更新了所有 32 个对手的每一个的 action and states，然后更新到 opponent_actions and opponent_rnn_states当中\n",
    "  * 最后将 actions and opponent_actions 拼接在一起\n",
    "\n",
    "* 返回：\n",
    "  * 函数返回值包括：状态值估计 values、合并后的动作 actions、动作对数概率 action_log_probs、以及代理的 RNN 状态 rnn_states_actor 和 rnn_states_critic\n",
    "\n",
    "该 collect 函数的核心是获取当前策略（包含代理和对手）的行为数据，并将这些数据按 rollout 线程数进行整理。这些数据可用于后续的策略更新或训练阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`init_states = {'ic_long_gc_deg': 120.0, 'ic_lat_geod_deg': 60.0, 'ic_h_sl_ft': 20000, 'ic_psi_true_deg': 0.0, 'ic_u_fps': 800.0}`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
